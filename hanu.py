# -*- coding: utf-8 -*-
"""hanu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sFcGaEv71ey7pHnhEsIzCV8pAm9uoykY
"""

# ===================== IMPORTS =====================
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
import numpy as np
from sklearn.metrics import roc_auc_score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===================== HYPERPARAMETERS =====================
LATENT_DIM = 16        # Try: 2, 8, 16, 32
BETA = 4.0             # Try: 0.1, 1, 5, 10
BATCH_SIZE = 128
EPOCHS = 20
LR = 1e-3
ANOMALY_CLASS = 0      # Class treated as anomaly (T-shirt)

# ===================== DATA =====================
transform = transforms.Compose([
    transforms.ToTensor()
])

train_data = datasets.FashionMNIST(
    root="./data", train=True, download=True, transform=transform
)

test_data = datasets.FashionMNIST(
    root="./data", train=False, download=True, transform=transform
)

# Train ONLY on normal data (exclude anomaly class)
normal_indices = [i for i, (_, y) in enumerate(train_data) if y != ANOMALY_CLASS]
train_subset = Subset(train_data, normal_indices)

train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)

# ===================== VAE MODEL =====================
class VAE(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28 * 28, 400),
            nn.ReLU()
        )
        self.mu = nn.Linear(400, latent_dim)
        self.logvar = nn.Linear(400, latent_dim)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 400),
            nn.ReLU(),
            nn.Linear(400, 28 * 28),
            nn.Sigmoid()
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        h = self.encoder(x)
        mu = self.mu(h)
        logvar = self.logvar(h)
        z = self.reparameterize(mu, logvar)
        x_hat = self.decoder(z)
        return x_hat, mu, logvar

# ===================== LOSS FUNCTION =====================
def vae_loss(x, x_hat, mu, logvar, beta):
    recon_loss = nn.functional.binary_cross_entropy(
        x_hat, x.view(-1, 28 * 28), reduction='sum'
    )
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + beta * kl_loss, recon_loss, kl_loss

# ===================== TRAINING =====================
model = VAE(LATENT_DIM).to(device)
optimizer = optim.Adam(model.parameters(), lr=LR)

model.train()
for epoch in range(EPOCHS):
    total_loss = 0
    for x, _ in train_loader:
        x = x.to(device)

        optimizer.zero_grad()
        x_hat, mu, logvar = model(x)
        loss, _, _ = vae_loss(x, x_hat, mu, logvar, BETA)

        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {total_loss / len(train_loader.dataset):.4f}")

# ===================== ANOMALY DETECTION =====================
model.eval()
recon_errors = []
labels = []

with torch.no_grad():
    for x, y in test_loader:
        x = x.to(device)
        x_hat, _, _ = model(x)

        error = torch.mean(
            (x.view(-1, 28*28) - x_hat) ** 2, dim=1
        )

        recon_errors.extend(error.cpu().numpy())
        labels.extend((y == ANOMALY_CLASS).numpy())

# ===================== AUC-ROC =====================
auc = roc_auc_score(labels, recon_errors)
print(f"\nAUC-ROC Score: {auc:.4f}")