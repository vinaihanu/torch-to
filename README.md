# torch-to
ğŸš€ Implementing and Optimizing Variational Autoencoders (VAEs) for Anomaly Detection
ğŸ“Œ Project Overview

This project implements a Variational Autoencoder (VAE) using PyTorch and applies it to an anomaly detection task on the Fashion-MNIST dataset.
The goal is to rigorously analyze how latent space dimensionality and Î²-VAE regularization affect reconstruction quality and anomaly detection performance.

Unlike standard autoencoders, VAEs model data probabilistically, making them highly suitable for outlier detection, density estimation, and uncertainty-aware learning, which are critical in domains such as fraud detection and industrial monitoring.

ğŸ¯ Objectives

Implement a VAE from scratch with correct reparameterization

Train the model only on normal data

Introduce anomalies during evaluation

Tune latent dimension and Î² (beta) hyperparameter

Evaluate anomaly detection performance using AUC-ROC

Analyze the trade-off between reconstruction accuracy and latent regularization

ğŸ§  Key Concepts Used

Variational Autoencoders (VAEs)

Reparameterization Trick

KL Divergence Regularization

Î²-VAE

Reconstruction Errorâ€“based Anomaly Detection

AUC-ROC Evaluation Metric

ğŸ“‚ Dataset

Fashion-MNIST

70,000 grayscale images (28Ã—28)

10 clothing categories

One class is treated as anomalous during testing

Model is trained only on normal classes

ğŸ— Model Architecture
Encoder

Input: 28Ã—28 image (flattened)

Dense layer â†’ ReLU

Outputs:

Mean (Î¼)

Log-variance (log ÏƒÂ²)

Latent Space

Dimension: configurable (2, 8, 16, 32)

Sampling via reparameterization trick

Decoder

Dense layers

Sigmoid output for image reconstruction

ğŸ”¢ Loss Function

The VAE loss is defined as:

ğ¿
=
Reconstruction Loss
+
ğ›½
â‹…
KL Divergence
L=Reconstruction Loss+Î²â‹…KL Divergence

Reconstruction Loss: Binary Cross Entropy

KL Divergence: Regularizes latent space towards a unit Gaussian

Î² (Beta): Controls the strength of regularization

âš™ Hyperparameter Tuning

The following parameters were systematically tuned:

Parameter	Values Explored
Latent Dimension	2, 8, 16, 32
Î² (Beta)	0.1, 1, 5, 10
Optimizer	Adam
Learning Rate	0.001
Epochs	20

Each configuration was evaluated using AUC-ROC to determine the optimal balance between reconstruction quality and anomaly separation.

ğŸš¨ Anomaly Detection Strategy

The VAE is trained only on normal data

During testing, anomalies are introduced

Reconstruction error (MSE) is used as the anomaly score

Higher reconstruction error â‡’ higher likelihood of anomaly

ğŸ“Š Evaluation Metric

AUC-ROC (Area Under the Receiver Operating Characteristic Curve) is used because:

It is threshold-independent

It handles class imbalance effectively

It is standard for anomaly detection tasks

ğŸ“ˆ Results & Observations

Low Î² values lead to excellent reconstruction but poor anomaly separation

High Î² values enforce stronger latent regularization but reduce reconstruction quality

An optimal Î² achieves the highest AUC-ROC, balancing both objectives

Moderate latent dimensions (e.g., 16) perform better than extremely small or large ones

ğŸ§© Trade-off Analysis
Aspect	Low Î²	High Î²
Reconstruction	Excellent	Poor
Latent Structure	Weak	Strong
Anomaly Detection	Weak	Strong
Overall Balance	âŒ	âœ… (Optimal range)
ğŸ›  How to Run the Project
pip install torch torchvision scikit-learn numpy

python vae_anomaly_detection.py

ğŸ“ Project Structure
â”œâ”€â”€ vae_anomaly_detection.py
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â””â”€â”€ results/

âœ… Conclusion

This project demonstrates that Variational Autoencoders, when properly regularized using Î²-VAE, are powerful tools for anomaly detection.
Through systematic hyperparameter tuning and principled evaluation, the model effectively distinguishes anomalous samples using reconstruction-based metrics.

The analysis highlights the importance of balancing reconstruction fidelity and latent space regularization to achieve optimal detection performance.

